{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc67b29f-d99c-44ba-aced-c585297c4582",
   "metadata": {},
   "source": [
    "# Audio2Blendshape test notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dad080d-ac1f-4bfe-9ecd-113f0a21dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsekai/mambaforge/envs/beat/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scripts.Dataset import a2bsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca3d4d-24cb-40fb-a784-bb5e09d868aa",
   "metadata": {},
   "source": [
    "## Initialize train-eval-test split\n",
    "Set *build_cache=True* to buid cache if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1690551-52a6-46f6-9fe1-133776eb2a45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = a2bsDataset(loader_type='train', build_cache=True)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "            train_data, \n",
    "            batch_size=16,  \n",
    "            shuffle=True,  \n",
    "            num_workers=0,\n",
    "            drop_last=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab74e364-ca58-4e8a-bb35-de3cc4ecf819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6460"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd7e56c8-2513-4878-b4d5-d67b4514f451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_data = a2bsDataset(loader_type='eval', build_cache=False)\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "            eval_data, \n",
    "            batch_size=16,  \n",
    "            shuffle=True,  \n",
    "            num_workers=0,\n",
    "            drop_last=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e08a393-9b6a-4fda-a42f-1e4a4318e36a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1447"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3fa3aa5-5110-4416-8fb4-9f02d656b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    '''\n",
    "    from timm\n",
    "    '''\n",
    "    def __init__(self, inplanes, planes, ker_size, stride=1, downsample=None, cardinality=1, base_width=64,\n",
    "                 reduce_first=1, dilation=1, first_dilation=None, act_layer=nn.LeakyReLU,   norm_layer=nn.BatchNorm1d, attn_layer=None, aa_layer=None, drop_block=None, drop_path=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            inplanes, planes, kernel_size=ker_size, stride=stride, padding=first_dilation,\n",
    "            dilation=dilation, bias=True)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.act1 = act_layer(inplace=True)\n",
    "        #self.aa = aa_layer(channels=first_planes, stride=stride) if use_aa else None\n",
    "\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            planes, planes, kernel_size=ker_size, padding=ker_size//2, dilation=dilation, bias=True)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "\n",
    "        #self.se = create_attn(attn_layer, outplanes)\n",
    "\n",
    "        self.act2 = act_layer(inplace=True)\n",
    "        if downsample is not None:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(inplanes, planes,  stride=stride, kernel_size=ker_size, padding=first_dilation, dilation=dilation, bias=True),\n",
    "                norm_layer(planes), \n",
    "            )\n",
    "        else: self.downsample=None\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.drop_block = drop_block\n",
    "        self.drop_path = drop_path\n",
    "\n",
    "    def zero_init_last_bn(self):\n",
    "        nn.init.zeros_(self.bn2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"x after 0\", x.shape)\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        #print(\"x after 1\", x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        #print(\"x after 2\", x.shape)\n",
    "        if self.downsample is not None:\n",
    "            shortcut = self.downsample(shortcut)\n",
    "        x += shortcut\n",
    "        x = self.act2(x)\n",
    "        #print(\"x after 3\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "752a52de-ca5f-49f9-bf43-f6f14f21a114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WavEncoder(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__() #128*1*140844 \n",
    "        self.out_dim = out_dim\n",
    "        self.feat_extractor = nn.Sequential( #b = (a+3200)/5 a \n",
    "                BasicBlock(1, 32, 15, 5, first_dilation=1600, downsample=True),\n",
    "                BasicBlock(32, 32, 15, 6, first_dilation=0, downsample=True),\n",
    "                BasicBlock(32, 32, 15, 1, first_dilation=7, ),\n",
    "                BasicBlock(32, 64, 15, 6, first_dilation=0, downsample=True),\n",
    "                BasicBlock(64, 64, 15, 1, first_dilation=7),\n",
    "                BasicBlock(64, 128, 15, 6,  first_dilation=0,downsample=True),     \n",
    "            )\n",
    "        \n",
    "    def forward(self, wav_data):\n",
    "        wav_data = wav_data.unsqueeze(1)  # add channel dim\n",
    "        out = self.feat_extractor(wav_data)\n",
    "        return out.transpose(1, 2)  # to (batch x seq x dim)\n",
    "\n",
    "class FaceGenerator(nn.Module):\n",
    "    def __init__(self, facial_dims = 51, audio_f = 128, hidden_size = 256, n_layer = 4, dropout_prob = 0.3):\n",
    "        super().__init__()\n",
    "        #self.pre_length = args.pre_frames #4\n",
    "        #self.gen_length = args.facial_length - args.pre_frames #30\n",
    "        self.facial_dims = facial_dims\n",
    "        #self.speaker_f = args.speaker_f\n",
    "        self.audio_f = audio_f\n",
    "        #self.facial_in = int(args.facial_rep[-2:])\n",
    "        \n",
    "        self.in_size = self.audio_f + self.facial_dims + 1\n",
    "        self.audio_encoder = WavEncoder(self.audio_f)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layer = n_layer\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # if self.facial_f is not 0:  \n",
    "        #     self.facial_encoder = nn.Sequential( #b = (a+3200)/5 a \n",
    "        #         BasicBlock(self.facial_in, self.facial_f//2, 7, 1, first_dilation=3,  downsample=True),\n",
    "        #         BasicBlock(self.facial_f//2, self.facial_f//2, 3, 1, first_dilation=1,  downsample=True),\n",
    "        #         BasicBlock(self.facial_f//2, self.facial_f//2, 3, 1, first_dilation=1, ),\n",
    "        #         BasicBlock(self.facial_f//2, self.facial_f, 3, 1, first_dilation=1,  downsample=True),   \n",
    "        #     )\n",
    "        # else:\n",
    "        #     self.facial_encoder = None\n",
    "\n",
    "        \n",
    "        self.gru = nn.GRU(self.in_size, hidden_size=self.hidden_size, num_layers=self.n_layer, batch_first=True,\n",
    "                          bidirectional=True, dropout=self.dropout_prob)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size//2),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(self.hidden_size//2, self.facial_dims)\n",
    "        )\n",
    "        \n",
    "        self.do_flatten_parameters = False\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.do_flatten_parameters = True\n",
    "            \n",
    "\n",
    "    def forward(self, pre_seq, in_audio, is_test=False): #pre_seq in this case is in_face\n",
    "        decoder_hidden = decoder_hidden_hands = None\n",
    "        \n",
    "        if self.do_flatten_parameters:\n",
    "            self.gru.flatten_parameters()\n",
    "\n",
    "        audio_feat_seq = None\n",
    "        audio_feat_seq = self.audio_encoder(in_audio)  # output (bs, n_frames, feat_size)\n",
    "        \n",
    "        # if self.facial_f is not 0:\n",
    "        #     # facial\n",
    "        #     # print(in_facial.shape)\n",
    "        #     face_feat_seq = self.facial_encoder(in_facial.permute([0, 2, 1]))\n",
    "        #     face_feat_seq = face_feat_seq.permute([0, 2, 1])\n",
    "    \n",
    "        # if is_test:\n",
    "        #     if self.facial_f is not 0:\n",
    "        #         min_length = min(pre_seq.shape[1], audio_feat_seq.shape[1], face_feat_seq.shape[1])\n",
    "        #         pre_seq = pre_seq[:,: min_length]\n",
    "        #         audio_feat_seq = audio_feat_seq[:, : min_length]\n",
    "        #         face_feat_seq = face_feat_seq[:, : min_length]\n",
    "        #     else:\n",
    "        #         min_length = min(pre_seq.shape[1], audio_feat_seq.shape[1])\n",
    "        #         pre_seq = pre_seq[:,: min_length]\n",
    "        #         audio_feat_seq = audio_feat_seq[:, : min_length]\n",
    "            #print(pre_seq.shape)\n",
    "        # if self.audio_f is not 0 and self.facial_f is 0:\n",
    "        #     in_data = torch.cat((pre_seq, audio_feat_seq), dim=2)\n",
    "        # elif self.audio_f is not 0 and self.facial_f is not 0:\n",
    "        #     in_data = torch.cat((pre_seq, audio_feat_seq, face_feat_seq), dim=2)\n",
    "        # else: pass\n",
    "        \n",
    "        in_data = torch.cat((pre_seq, audio_feat_seq), dim=2)\n",
    "        \n",
    "        # if speaker_feat_seq is not None:\n",
    "        #     #if print(z_context.shape)\n",
    "        #     repeated_s = speaker_feat_seq\n",
    "        #     #print(repeated_s.shape)\n",
    "        #     if len(repeated_s.shape) == 2:\n",
    "        #         repeated_s = repeated_s.reshape(1, repeated_s.shape[1], repeated_s.shape[0])\n",
    "        #         #print(repeated_s.shape)\n",
    "        #     repeated_s = repeated_s.repeat(1, in_data.shape[1], 1)\n",
    "        #     #print(repeated_s.shape)\n",
    "        #     #print(repeated_s.shape)\n",
    "        #     in_data = torch.cat((in_data, repeated_s), dim=2)\n",
    "        \n",
    "        \n",
    "        output, decoder_hidden = self.gru(in_data, decoder_hidden)\n",
    "        output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]  # sum bidirectional outputs\n",
    "        output = self.out(output.reshape(-1, output.shape[2]))\n",
    "        decoder_outputs = output.reshape(in_data.shape[0], in_data.shape[1], -1)\n",
    "\n",
    "\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36e9f17e-bf6f-446b-b45d-bb8ae6fd604d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_audio, facial, in_id = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61afad2a-17bd-4579-8dc1-6660a102e859",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 36266]) torch.Size([16, 34, 51]) torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "print(in_audio.shape, facial.shape, in_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8df781ee-5f0c-4c6e-b23f-a263024c7c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = FaceGenerator().cuda()\n",
    "optimizer = torch.optim.Adam( net.parameters(), lr=1e-3)\n",
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))\n",
    "loss_function = RMSLELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "37a33cf8-d964-4267-a548-0d33aab49d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0][0/535] loss: 0.05445299297571182\n",
      "[0][100/535] loss: 0.05884157121181488\n",
      "[0][200/535] loss: 0.05473871901631355\n",
      "[0][300/535] loss: 0.059451572597026825\n",
      "[0][400/535] loss: 0.05230134725570679\n",
      "[0][500/535] loss: 0.049990464001894\n",
      "[1][0/535] loss: 0.06423146277666092\n",
      "[1][100/535] loss: 0.062476251274347305\n",
      "[1][200/535] loss: 0.053438518196344376\n",
      "[1][300/535] loss: 0.054198652505874634\n",
      "[1][400/535] loss: 0.062087081372737885\n",
      "[1][500/535] loss: 0.05568043887615204\n",
      "[2][0/535] loss: 0.05325791984796524\n",
      "[2][100/535] loss: 0.05018600821495056\n",
      "[2][200/535] loss: 0.05615130811929703\n",
      "[2][300/535] loss: 0.05180227756500244\n",
      "[2][400/535] loss: 0.05200745910406113\n",
      "[2][500/535] loss: 0.06145203486084938\n",
      "[3][0/535] loss: 0.05457715317606926\n",
      "[3][100/535] loss: 0.04981430992484093\n",
      "[3][200/535] loss: 0.04949994385242462\n",
      "[3][300/535] loss: 0.049588557332754135\n",
      "[3][400/535] loss: 0.06405346840620041\n",
      "[3][500/535] loss: 0.05206301808357239\n",
      "[4][0/535] loss: 0.05488245189189911\n",
      "[4][100/535] loss: 0.056847698986530304\n",
      "[4][200/535] loss: 0.05461922660470009\n",
      "[4][300/535] loss: 0.05407250300049782\n",
      "[4][400/535] loss: 0.04882293567061424\n",
      "[4][500/535] loss: 0.05741643160581589\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "num_epochs = 5\n",
    "log_period = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for it, (in_audio, facial, in_id) in enumerate(train_loader):\n",
    "        in_audio = in_audio.cuda()\n",
    "        facial = facial.cuda()\n",
    "        pre_frames = 4\n",
    "        in_pre_face = facial.new_zeros((facial.shape[0], facial.shape[1], facial.shape[2] + 1)).cuda()\n",
    "        in_pre_face[:, 0:pre_frames, :-1] = facial[:, 0:pre_frames]\n",
    "        in_pre_face[:, 0:pre_frames, -1] = 1 \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out_face = net(in_pre_face,in_audio)\n",
    "        loss = loss_function(facial, out_face)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #logging\n",
    "        if it % log_period == 0:\n",
    "            print(f'[{epoch}][{it}/{len(train_loader)}] loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2333099c-f3e8-4546-ae57-7e3ae43f2400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "in_audio, facial, in_id = next(iter(eval_loader))\n",
    "in_audio = in_audio.cuda()\n",
    "facial = facial.cuda()\n",
    "pre_frames = 4\n",
    "in_pre_face = facial.new_zeros((facial.shape[0], facial.shape[1], facial.shape[2] + 1)).cuda()\n",
    "in_pre_face[:, 0:pre_frames, :-1] = facial[:, 0:pre_frames]\n",
    "in_pre_face[:, 0:pre_frames, -1] = 1 \n",
    "out_face = net(in_pre_face, in_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3fe2dedf-c4c0-4546-bb01-5ecbb7bdface",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 34, 51]), torch.Size([1, 34, 51]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_face.shape, facial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8f061314-8b0f-4ec3-b02b-f1555d32f101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2166, 0.2283, 0.2397, 0.2315, 0.2428, 0.2493, 0.2504, 0.2541, 0.2573,\n",
       "        0.2609, 0.2718, 0.2830, 0.2924, 0.2983, 0.2963, 0.2858, 0.2360, 0.2100,\n",
       "        0.1918, 0.2012, 0.2473, 0.2794, 0.2949, 0.2902, 0.2707, 0.2000, 0.2018,\n",
       "        0.2292, 0.2660, 0.3031, 0.3274, 0.3392, 0.3476, 0.3398],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_face[0,:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "50e540b0-902a-4047-8e30-8a5dea1f59c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2018, 0.1870, 0.1821, 0.1815, 0.1819, 0.1816, 0.1878, 0.1939, 0.1885,\n",
       "        0.1680, 0.1425, 0.1257, 0.1093, 0.0941, 0.0808, 0.0839, 0.1211, 0.1340,\n",
       "        0.1208, 0.1187, 0.1301, 0.1508, 0.1523, 0.1383, 0.1254, 0.1011, 0.0942,\n",
       "        0.1182, 0.1292, 0.1447, 0.1581, 0.1300, 0.0912, 0.0982],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facial[0,:,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec431f-4e4c-438e-8d53-dc808368aaff",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a812175-621f-4df3-b93b-3593c1d5f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = a2bsDataset(loader_type='test', build_cache=False)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            test_data, \n",
    "            batch_size=16,  \n",
    "            shuffle=True,  \n",
    "            num_workers=0,\n",
    "            drop_last=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aa084dd-6867-4724-b460-84841974e577",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
